# Multi-Source-RAG-Application-with-LangChain-Google-Gemini
This project demonstrates a powerful Retrieval-Augmented Generation (RAG) application built using Python, LangChain, and Streamlit. It enhances the capabilities of Google's Gemini LLM by providing it with real-time, external context from web pages (URLs) and PDF documents.

The application allows users to ask questions about the content of a specific URL or a PDF file, showcasing the difference between a standard LLM response and a context-aware RAG response.

# LangChain & Google Gemini ile Ã‡ok KaynaklÄ± RAG UygulamasÄ±

Bu proje, Python, LangChain ve Streamlit kullanÄ±larak oluÅŸturulmuÅŸ gÃ¼Ã§lÃ¼ bir Geri-Getirme Destekli Ãœretim (Retrieval-Augmented Generation - RAG) uygulamasÄ±nÄ± sergilemektedir. Google'Ä±n Gemini dil modelinin yeteneklerini, web sayfalarÄ±ndan (URL) ve PDF dokÃ¼manlarÄ±ndan alÄ±nan gerÃ§ek zamanlÄ±, harici baÄŸlamlarla zenginleÅŸtirir.

Uygulama, kullanÄ±cÄ±larÄ±n belirli bir URL'nin veya PDF dosyasÄ±nÄ±n iÃ§eriÄŸi hakkÄ±nda sorular sormasÄ±na olanak tanÄ±r ve standart bir dil modeli yanÄ±tÄ± ile baÄŸlama duyarlÄ± bir RAG yanÄ±tÄ± arasÄ±ndaki farkÄ± gÃ¶zler Ã¶nÃ¼ne serer.

# Key Features

**Dual Data Sources:** Processes information from both live web pages (URLs) and local PDF files.

**RAG vs. Standard LLM:** Provides a side-by-side comparison of answers generated with and without external context, clearly demonstrating the power of RAG in preventing hallucinations and providing factual responses.

**Multi-Model Integration:** Utilizes **Google Gemini (gemini-1.5-flash-latest)** for text generation and **OpenAI's Embeddings** for text vectorization, showcasing flexibility in using different AI services.

**Efficient Vector Search:** Employs **FAISS (Facebook AI Similarity Search)** for fast and efficient in-memory similarity searches on document vectors.

**Interactive Web UI:** A user-friendly interface built with **Streamlit** allows for easy interaction with the system.

# ğŸ› ï¸ Tech Stack & Architecture

This project is built on a modern Generative AI stack:

* Framework: LangChain
* LLM (Generator): Google Gemini
* Embeddings Model: OpenAI
* Vector Store: FAISS
* UI: Streamlit
* Document Loaders: WebBaseLoader, PyPDFLoader

# How It Works (RAG Pipeline)

1. Load: The application loads data from the user-provided URL or PDF file.
2. Split: The raw text is split into smaller, manageable chunks using 3. RecursiveCharacterTextSplitter.
3. Embed: Each chunk is converted into a numerical vector using OpenAI's embedding model.
4. Store: These vectors are stored in a FAISS in-memory vector database.
5. Retrieve: When a user asks a question, the application embeds the query and uses FAISS to find the most semantically similar document chunks (the "context").
6. Generate: The original prompt and the retrieved context are passed to the Gemini LLM. The model then generates a final, context-aware answer based only on the provided information.

# Setup and Usage

## Create and Activate a Virtual Environment
```
# Windows
python -m venv venv
venv\Scripts\activate

# macOS / Linux
python3 -m venv venv
source venv/bin/activate
```

## Set Up Environment Variables

* Create a .env file in the root directory of the project
* Open the .env file and add your API keys from OpenAI and Google AI Studio.

## Prepare the Data Directory (for PDF)

The application expects uploaded PDFs to be temporarily handled. For a seamless experience with the provided code structure (filepath=f"./data/{selected_file.name}"), ensure a data folder exists in the root directory. Note: Streamlit's file uploader handles this in memory, but having a data folder is good practice if you wish to save files permanently.

## Run the Streamlit Application

```python
streamlit run app.py
```

Open your web browser and navigate to the local URL provided by Streamlit (usually http://localhost:8501).


TR

# Ã–ne Ã‡Ä±kan Ã–zellikler

**Ä°kili Veri KaynaÄŸÄ±:** Hem canlÄ± web sayfalarÄ±ndan (URL) hem de yerel PDF dosyalarÄ±ndan gelen bilgileri iÅŸler.
**RAG ve Standart LLM KarÅŸÄ±laÅŸtÄ±rmasÄ±:** Harici baÄŸlam kullanÄ±larak ve kullanÄ±lmadan Ã¼retilen yanÄ±tlarÄ± yan yana karÅŸÄ±laÅŸtÄ±rarak, RAG'in halÃ¼sinasyonlarÄ± Ã¶nlemedeki ve gerÃ§eÄŸe dayalÄ± yanÄ±tlar vermedeki gÃ¼cÃ¼nÃ¼ net bir ÅŸekilde gÃ¶sterir.
**Ã‡oklu Model Entegrasyonu:** Metin Ã¼retimi iÃ§in Google Gemini (gemini-1.5-flash-latest) ve metin vektÃ¶rleÅŸtirmesi iÃ§in OpenAI Embeddings kullanarak farklÄ± yapay zeka servislerini bir arada kullanma esnekliÄŸini sergiler.
**Etkin VektÃ¶r AramasÄ±:** DokÃ¼man vektÃ¶rleri Ã¼zerinde hÄ±zlÄ± ve verimli bellek-iÃ§i benzerlik aramalarÄ± iÃ§in FAISS (Facebook AI Similarity Search) kullanÄ±r.
**EtkileÅŸimli Web ArayÃ¼zÃ¼:** Streamlit ile oluÅŸturulmuÅŸ kullanÄ±cÄ± dostu bir arayÃ¼z, sistemle kolay etkileÅŸim saÄŸlar.

# Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

## Sanal Ortam OluÅŸturun ve AktifleÅŸtirin
```
# Windows
python -m venv venv
venv\Scripts\activate

# macOS / Linux
python3 -m venv venv
source venv/bin/activate
```

## Ortam DeÄŸiÅŸkenlerini AyarlayÄ±n

Projenin kÃ¶k dizininde, bir .env dosyasÄ± oluÅŸturun.

.env dosyasÄ±nÄ± aÃ§Ä±n ve OpenAI ve Google AI Studio'dan aldÄ±ÄŸÄ±nÄ±z API anahtarlarÄ±nÄ±zÄ± ekleyin.

## Veri KlasÃ¶rÃ¼nÃ¼ HazÄ±rlayÄ±n (PDF iÃ§in)

Uygulama, yÃ¼klenen PDF'lerin geÃ§ici olarak iÅŸlenmesini bekler. Mevcut kod yapÄ±sÄ±yla (filepath=f"./data/{selected_file.name}") sorunsuz bir deneyim iÃ§in kÃ¶k dizinde bir data klasÃ¶rÃ¼nÃ¼n bulunduÄŸundan emin olun. Not: Streamlit'in dosya yÃ¼kleyicisi bu iÅŸlemi bellekte yÃ¶netir, ancak dosyalarÄ± kalÄ±cÄ± olarak kaydetmek isterseniz bir data klasÃ¶rÃ¼ne sahip olmak iyi bir pratiktir.

## Streamlit UygulamasÄ±nÄ± Ã‡alÄ±ÅŸtÄ±rÄ±n

```python
streamlit run app.py
```
Web tarayÄ±cÄ±nÄ±zÄ± aÃ§Ä±n ve Streamlit tarafÄ±ndan saÄŸlanan yerel URL'ye gidin (genellikle http://localhost:8501).

![Uygulama ArayÃ¼zÃ¼](https://github.com/Mervecaliskann/Multi-Source-RAG-Application-with-LangChain-Google-Gemini/blob/main/langchain-bellek-genisletme-url-pdf-Ekran%20g%C3%B6r%C3%BCnt%C3%BCs%C3%BC%202025-08-06%20232519.png)


